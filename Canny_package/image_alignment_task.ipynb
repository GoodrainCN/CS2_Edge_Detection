{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import scipy.misc as sm\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description\n",
    "\n",
    "#### Input image loading\n",
    "\n",
    "The input image is the set of 3 plates, corresponding to B, G, and R channels (top-down). You should implement the function $\\tt{load}$\\_$\\tt{data}$ that reads the data and returns the list of images of plates.\n",
    "$\\tt{dir}$\\_$\\tt{name}$ is the path to the directory with plate images. If this directory is located in the same directory as this notebook, then default arguments can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_name = 'faces_imgs'):    \n",
    "    '''\n",
    "    Load images from the \"faces_imgs\" directory\n",
    "    Images are in JPG and we convert it to gray scale images\n",
    "    '''\n",
    "    imgs = []\n",
    "    for filename in os.listdir(dir_name):\n",
    "        if filename == '.DS_Store':\n",
    "            continue\n",
    "        if filename == \"archive\":\n",
    "            continue\n",
    "        img = mpimg.imread(dir_name + '/' + filename)\n",
    "        #img = skimage.color.rgb2gray(img)\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "    \n",
    "plates = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a list of 2-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The auxiliary function `visualize()` displays the images given as argument.\n",
    "def visualize(imgs, format=None):\n",
    "    plt.figure(figsize=(20, 40))\n",
    "    for i, img in enumerate(imgs):\n",
    "        if img.shape[0] == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "        plt_idx = i+1\n",
    "        plt.subplot(4, 2, plt_idx)    \n",
    "        plt.imshow(img, vmin=0, vmax=255)\n",
    "    plt.show()\n",
    "\n",
    "visualize(plates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The borders removal (1.5 points)\n",
    "It is worth noting that there is a framing from all sides in most of the images. This framing can appreciably worsen the quality of channels alignment. Here, we suggest that you find the borders on the plates using Canny edge detector, and crop the images according to these edges. The example of using Canny detector implemented in skimage library can be found [here](http://scikit-image.org/docs/dev/auto_examples/edges/plot_canny.html).<br>\n",
    "\n",
    "The borders can be removed in the following way:\n",
    "* Apply Canny edge detector to the image.\n",
    "* Find the rows and columns of the frame pixels. \n",
    "For example, in case of upper bound we will search for the row in the neighborhood of the upper edge of the image (e.g. 5% of its height). For each row let us count the number of edge pixels (obtained using Canny detector) it contains. Having these number let us find two maximums among them. Two rows corresponding to these maximums are edge rows. As there are two color changes in the frame (firstly, from light scanner background to the dark tape and then from the tape to the image), we need the second maximum that is further from the image border. The row corresponding to this maximum is the crop border. In order not to find two neighboring peaks, non-maximum suppression should be implemented: the rows next to the first maximum are set to zero, and after that, the second maximum is searched for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canny detector implementation (2.5 points)\n",
    "You can write your own implementation of Canny edge detector to get extra points. <br>\n",
    "\n",
    "Canny detection algorithm:\n",
    "1. *Noise reduction.* To remove noise, the image is smoothed by Gaussian blur with the kernel of size $5 \\times 5$ and $\\sigma = 1.4$. Since the sum of the elements in the Gaussian kernel equals $1$, the kernel should be normalized before the convolution. <br><br>\n",
    "\n",
    "2. *Calculating gradients.* When the image $I$ is smoothed, the derivatives $I_x$ and $I_y$ w.r.t. $x$ and $y$ are calculated. It can be implemented by convolving $I$ with Sobel kernels $K_x$ and $K_y$, respectively: \n",
    "$$ K_x = \\begin{pmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{pmatrix}, K_y = \\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{pmatrix}. $$ \n",
    "Then, the magnitude $G$ and the slope $\\theta$ of the gradient are calculated:\n",
    "$$ |G| = \\sqrt{I_x^2 + I_y^2}, $$\n",
    "$$ \\theta(x,y) = arctan\\left(\\frac{I_y}{I_x}\\right)$$<br><br>\n",
    "\n",
    "3. *Non-maximum suppression.* For each pixel find two neighbors (in the positive and negative gradient directions, supposing that each neighbor occupies the angle of $\\pi /4$, and $0$ is the direction straight to the right). If the magnitude of the current pixel is greater than the magnitudes of the neighbors, nothing changes, otherwise, the magnitude of the current pixel is set to zero.<br><br>\n",
    "\n",
    "4. *Double threshold.* The gradient magnitudes are compared with two specified threshold values, the first one is less than the second. The gradients that are smaller than the low threshold value are suppressed; the gradients higher than the high threshold value are marked as strong ones and the corresponding pixels are included in the final edge map. All the rest gradients are marked as weak ones and pixels corresponding to these gradients are considered in the next step.<br><br>\n",
    "\n",
    "5. *Edge tracking by hysteresis.* Since a weak edge pixel caused from true edges will be connected to a strong edge pixel, pixel $w$ with weak gradient is marked as edge and included in the final edge map if and only if it is involved in the same blob (connected component) as some pixel $s$ with strong gradient. In other words, there should be a chain of neighbor weak pixels connecting $w$ and $s$ (the neighbors are 8 pixels around the considered one). You are welcome to make up and implement an algorithm that finds all the connected components of the gradient map considering each pixel only once.  After that, you can decide which pixels will be included in the final edge map (this algorithm should be single-pass, as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "def gaussian_kernel(size, sigma=1):\n",
    "    size = int(size) // 2\n",
    "    x, y = np.mgrid[-size:size+1, -size:size+1]\n",
    "    normal = 1 / (2.0 * np.pi * sigma**2)\n",
    "    g =  np.exp(-((x**2 + y**2) / (2.0*sigma**2))) * normal\n",
    "    return g\n",
    "\n",
    "\n",
    "def sobel_filters(img):\n",
    "    Kx = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], np.float32)\n",
    "    Ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], np.float32)\n",
    "    \n",
    "    Ix = ndimage.filters.convolve(img, Kx)\n",
    "    Iy = ndimage.filters.convolve(img, Ky)\n",
    "    \n",
    "    G = np.hypot(Ix, Iy)\n",
    "    G = G / G.max() * 255\n",
    "    theta = np.arctan2(Iy, Ix)\n",
    "    \n",
    "    return (G, theta)\n",
    "    \n",
    "\n",
    "def non_max_suppression(img, D):\n",
    "    M, N = img.shape\n",
    "    Z = np.zeros((M,N), dtype=np.int32)\n",
    "    angle = D * 180. / np.pi\n",
    "    #pi_4 = np.pi / 4\n",
    "    #pi_2 = np.pi / 2\n",
    "    angle[angle < 0] += 180\n",
    "\n",
    "    \n",
    "    for i in range(1,M-1):\n",
    "        for j in range(1,N-1):\n",
    "            try:\n",
    "                #theta = D[i,j] #* 180 / np.pi #angle in degrees\n",
    "                #theta_mod = theta % np.pi\n",
    "                q = 255\n",
    "                r = 255\n",
    "                #alpha = None\n",
    "                \n",
    "               #angle 0\n",
    "                if (0 <= angle[i,j] < 22.5) or (157.5 <= angle[i,j] <= 180):\n",
    "                    q = img[i, j+1]\n",
    "                    r = img[i, j-1]\n",
    "                #angle 45\n",
    "                elif (22.5 <= angle[i,j] < 67.5):\n",
    "                    q = img[i+1, j-1]\n",
    "                    r = img[i-1, j+1]\n",
    "                #angle 90\n",
    "                elif (67.5 <= angle[i,j] < 112.5):\n",
    "                    q = img[i+1, j]\n",
    "                    r = img[i-1, j]\n",
    "                #angle 135\n",
    "                elif (112.5 <= angle[i,j] < 157.5):\n",
    "                    q = img[i-1, j-1]\n",
    "                    r = img[i+1, j+1]\n",
    "\n",
    "                \"\"\"\n",
    "                if (0 <= theta_mod < pi_4):\n",
    "                    alpha = np.abs(np.tan(theta_mod))\n",
    "                    q = (alpha * img[i + 1, j + 1]) + ((1 - alpha) * img[i, j + 1])\n",
    "                    r = (alpha * img[i - 1, j - 1]) + ((1 - alpha) * img[i, j - 1]) \n",
    "                    \n",
    "                elif (pi_4 <= theta_mod < pi_2):\n",
    "                    alpha = np.abs(1./np.tan(theta_mod))\n",
    "                    q = (alpha * img[i + 1, j + 1]) + ((1 - alpha) * img[i + 1, j])\n",
    "                    r = (alpha * img[i - 1, j - 1]) + ((1 - alpha) * img[i - 1, j])\n",
    "                    \n",
    "                elif (pi_2 <= theta_mod < (3*pi_4)):\n",
    "                    alpha = np.abs(1./np.tan(theta_mod))\n",
    "                    q = (alpha * img[i + 1, j - 1]) + ((1 - alpha) * img[i + 1, j])\n",
    "                    r = (alpha * img[i - 1, j + 1]) + ((1 - alpha) * img[i - 1, j])\n",
    "                \n",
    "                elif ((3*pi_4) <= theta_mod < np.pi):\n",
    "                    alpha = np.abs(np.tan(theta_mod))\n",
    "                    q = (alpha * img[i + 1, j - 1]) + ((1 - alpha) * img[i, j - 1])\n",
    "                    r = (alpha * img[i - 1, j + 1]) + ((1 - alpha) * img[i, j + 1])\n",
    "                \"\"\"\n",
    "                if (img[i,j] >= q) and (img[i,j] >= r):\n",
    "                    Z[i,j] = img[i,j]\n",
    "                else:\n",
    "                    Z[i,j] = 0\n",
    "                \n",
    "\n",
    "            except IndexError as e:\n",
    "                pass\n",
    "    \n",
    "    return Z\n",
    "\n",
    "def threshold(img, lowThresholdRatio=0.05, highThresholdRatio=0.09):\n",
    "    \n",
    "    highThreshold = img.max() * highThresholdRatio;\n",
    "    lowThreshold = highThreshold * lowThresholdRatio;\n",
    "    \n",
    "    M, N = img.shape\n",
    "    res = np.zeros((M,N), dtype=np.int32)\n",
    "    \n",
    "    weak = np.int32(25)\n",
    "    strong = np.int32(255)\n",
    "    \n",
    "    strong_i, strong_j = np.where(img >= highThreshold)\n",
    "    zeros_i, zeros_j = np.where(img < lowThreshold)\n",
    "    \n",
    "    weak_i, weak_j = np.where((img <= highThreshold) & (img >= lowThreshold))\n",
    "    \n",
    "    res[strong_i, strong_j] = strong\n",
    "    res[weak_i, weak_j] = weak\n",
    "    \n",
    "    return (res, weak, strong)\n",
    "\n",
    "def hysteresis(img, weak, strong=255):\n",
    "    \n",
    "    M, N = img.shape  \n",
    "    \n",
    "    for i in range(1, M-1):\n",
    "        for j in range(1, N-1):\n",
    "            if (img[i,j] == weak):\n",
    "                try:\n",
    "                    if ((img[i+1, j-1] == strong) or (img[i+1, j] == strong) or (img[i+1, j+1] == strong)\n",
    "                        or (img[i, j-1] == strong) or (img[i, j+1] == strong)\n",
    "                        or (img[i-1, j-1] == strong) or (img[i-1, j] == strong) or (img[i-1, j+1] == strong)):\n",
    "                        img[i, j] = strong\n",
    "                    else:\n",
    "                        img[i, j] = 0\n",
    "                except IndexError as e:\n",
    "                    pass\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# TODO: implement Canny detector yourself.                      #\n",
    "#       You can use methods from scipy.ndimage if you need.     #\n",
    "#################################################################\n",
    "from  skimage.feature import canny\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage.filters import convolve\n",
    "\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def Canny_detector(img):\n",
    "    \"\"\" Your implementation instead of skimage \"\"\"\n",
    "    \n",
    "    img_filtered = convolve(img, gaussian_kernel(5, sigma=1.4))\n",
    "    grad, theta = sobel_filters(img_filtered)\n",
    "    img_nms = non_max_suppression(grad, theta)\n",
    "    img_thresh, weak, strong = threshold(img_nms, lowThresholdRatio=0.07, highThresholdRatio=0.19)\n",
    "    img_final = hysteresis(img_thresh, weak, strong=strong)\n",
    "   \n",
    "    return img_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canny_imgs = []\n",
    "for i, img in enumerate(plates):\n",
    "    print(\"Processing image %i\" % (i+1))\n",
    "    canny_img = Canny_detector(img)\n",
    "    canny_imgs.append(canny_img)\n",
    "    \n",
    "visualize(canny_imgs, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(-np.sum(canny_imgs[0][0:22,:], axis=1))[1]\n",
    "#canny_imgs[0][:,-57:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.flip(np.sum(canny_imgs[0][:,-22:], axis=0), axis=0)/255)\n",
    "print(np.argsort(-np.flip(np.sum(canny_imgs[0][:,-22:], axis=0), axis=0)))\n",
    "print(np.argsort(-np.sum(canny_imgs[0][:,-22:-12], axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(np.sum(canny_img[:,-44:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# TODO: Implement the removal of the tape borders using         #\n",
    "#       the output of Canny edge detector (\"canny_img\" list)    #\n",
    "#################################################################\n",
    "\n",
    "def remove_borders(img, canny_img):\n",
    "    \"\"\" Your implementation instead of the following one\"\"\"   \n",
    "    print(img.shape)\n",
    "    ys, xs = img.shape\n",
    "    dy = int(img.shape[0] * 0.05)\n",
    "    dx = int(img.shape[1] * 0.1)\n",
    "    print(dy, dx)\n",
    "\n",
    "    y_indices = np.argsort(-np.sum(canny_img[0:dy,:], axis=-1))\n",
    "    f_indices_upper_y = y_indices[0]\n",
    "    indices_upper_y = y_indices[y_indices > f_indices_upper_y+1][0]\n",
    "    \n",
    "    y_indices = np.argsort(-np.flip(np.sum(canny_img[-dy:,:], axis=-1), axis=0))\n",
    "    f_indices_lower_y = y_indices[0]\n",
    "    indices_lower_y = y_indices[y_indices > f_indices_lower_y+1][0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_indices = np.argsort(-np.sum(canny_img[:,0:dx], axis=0))\n",
    "    f_indices_left_x = x_indices[0]\n",
    "    if len(x_indices) == (f_indices_left_x + 1):\n",
    "        indices_left_x = f_indices_left_x\n",
    "    else:\n",
    "        indices_left_x = x_indices[x_indices > f_indices_left_x+1][0]\n",
    "\n",
    "    \n",
    "    x_indices = np.argsort(-np.flip(np.sum(canny_img[:,-dx:], axis=0), axis=0))\n",
    "    f_indices_right_x = x_indices[0]\n",
    "    if len(x_indices) == (f_indices_right_x + 1):\n",
    "        indices_right_x = f_indices_right_x\n",
    "    else:\n",
    "        indices_right_x = x_indices[x_indices > f_indices_right_x+1][0]\n",
    "    \n",
    "    \n",
    "    #print(indices_upper_y , indices_lower_y, indices_left_x , indices_right_x)\n",
    "    #print(\"new dims => \", img[indices_upper_y : -indices_lower_y, indices_left_x : -indices_right_x].shape)\n",
    "    return img[indices_upper_y : -indices_lower_y, indices_left_x : -indices_right_x]\n",
    "\n",
    "\n",
    "\n",
    "cropped_imgs = []\n",
    "#crop borders\n",
    "for i, img in enumerate(plates):\n",
    "    cropped_imgs.append(remove_borders(img, canny_imgs[i]))\n",
    "\n",
    "visualize(cropped_imgs, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels separation  (0.5 points)\n",
    "\n",
    "The next step is to separate the image into three channels (B, G, R) and make one colored picture. To get channels, you can divide each plate into three equal parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# TODO: implement the function impose_components transforming   #\n",
    "#       cropped black-and-white images cropped_imgs             #\n",
    "#       into the list of RGB images rgb_imgs                    #\n",
    "#################################################################\n",
    "def impose_components(img):\n",
    "    \"\"\" Your implementation \"\"\"   \n",
    "    h, w =img.shape\n",
    "    new_h = h // 3\n",
    "    img = img[:h-(h % new_h),:]\n",
    "    new_img = np.zeros((h//3, w, 3), dtype=np.int32)\n",
    "    new_img[:,:,2] = img[:new_h,:]\n",
    "    new_img[:,:,1] = img[new_h:2*new_h,:]\n",
    "    new_img[:,:,0] = img[2*new_h:,:]\n",
    "    \n",
    "    return new_img\n",
    "    \n",
    "\n",
    "\n",
    "rgb_imgs = []\n",
    "for cropped_img in cropped_imgs:\n",
    "    rgb_img = impose_components(cropped_img)\n",
    "    rgb_imgs.append(rgb_img)\n",
    "\n",
    "visualize(rgb_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for the best shift for channel alignment (1 point for metrics implementation + 2 points for channel alignment)\n",
    "\n",
    "In order to align two images, we will shift one image relative to another within some limits (e.g. from $-15$ to $15$ pixels). For each shift, we can calculate some metrics in the overlap of the images. Depending on the metrics, the best shift is the one the metrics achieves the greatest or the smallest value for. We suggest that you implement two metrics and choose the one that allows to obtain the better alignment quality:\n",
    "\n",
    "* *Mean squared error (MSE):*<br><br>\n",
    "$$ MSE(I_1, I_2) = \\dfrac{1}{w * h}\\sum_{x,y}(I_1(x,y)-I_2(x,y))^2, $$<br> where *w, h* are width and height of the images, respectively. To find the optimal shift you should find the minimum MSE over all the shift values.\n",
    "    <br><br>\n",
    "* *Normalized cross-correlation (CC):*<br><br>\n",
    "    $$\n",
    "    I_1 \\ast I_2 = \\dfrac{\\sum_{x,y}I_1(x,y)I_2(x,y)}{\\sum_{x,y}I_1(x,y)\\sum_{x,y}I_2(x,y)}.\n",
    "    $$<br>\n",
    "    To find the optimal shift you should find the maximum CC over all the shift values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# TODO: implement the functions mse Ð¸ cor calculating           #\n",
    "#       mean squared error and normalized cross-correlation     #\n",
    "#       for two input images, respectively (1 point)            #\n",
    "#################################################################\n",
    "def mse(X, Y):\n",
    "    xh, xw = X.shape\n",
    "    yh, yw = Y.shape\n",
    "    \n",
    "  \n",
    "    assert ((xw == yw) or (xh == yh)), \\\n",
    "    \"Arguments dimensions are not equals: %s not equal to %s\" % (str(xw), str(yw))\n",
    "    a = [((X[i,j] - Y[i,j])**2) for i in range(xh) for j in range(xw)]\n",
    "    return np.sum(a) / (xw * xh) \n",
    "\n",
    "def cor(X, Y):    \n",
    "    xh, xw = X.shape\n",
    "    yh, yw = Y.shape\n",
    "    \n",
    "    assert ((xw == yw) or (xh == yh)), \\\n",
    "    \"Arguments dimensions are not equals: %s not equal to %s\" % (str(xw), str(yw))\n",
    "    #print(np.multiply(X, Y).shape)\n",
    "    \n",
    "    return np.sum(np.multiply(X, Y)) / (np.sum(X)*np.sum(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# TODO: implement the algorithm to find the best channel        #\n",
    "#       shift and the alignment. Apply this algorithm for       #\n",
    "#       rgb_imgs processing and get the final list of colored   #\n",
    "#       pictures. These images will be used for the evaluation  #\n",
    "#       of the quality of the whole algorithm.  (2 points)      #\n",
    "#                                                               #\n",
    "#       You can use the following interface or write your own.  #\n",
    "#################################################################\n",
    "\n",
    "def shift_img(img, shift=0):\n",
    "    \n",
    "    img_shifted = np.zeros(img.shape, dtype=np.uint8)\n",
    "    \n",
    "    if (shift < 0):\n",
    "        img_shifted[shift:,:] = 0\n",
    "        img_shifted[:shift,:] = img[-shift:,:]    \n",
    "    elif (shift > 0):\n",
    "        img_shifted[:shift,:] = 0\n",
    "        img_shifted[shift:,:] = img[:-shift,:]\n",
    "    else:\n",
    "        img_shifted = img\n",
    "    \n",
    "    return img_shifted\n",
    "\n",
    "def get_best_shift(channel1, channel2, shift_max=30):\n",
    "    \n",
    "    img_shifted = np.zeros((channel1.shape), dtype=np.uint8)\n",
    "    \n",
    "    best_mse, best_cor = +1e6, -1000\n",
    "    best_shift_cor, best_shift_mse = None, None\n",
    "    \n",
    "    for shift in range(-shift_max, shift_max+1):\n",
    "        img_shifted = shift_img(channel1, shift)\n",
    "        \n",
    "        mse_ = mse(img_shifted, channel2)\n",
    "        cor_ = cor(img_shifted, channel2)\n",
    "        \n",
    "        if mse_ < best_mse:\n",
    "            best_mse = mse_\n",
    "            best_shift_mse = shift\n",
    "        if cor_ > best_cor:\n",
    "            best_cor = cor_\n",
    "            best_shift_cor = shift\n",
    "        \n",
    "    return (best_shift_mse, best_mse, best_shift_cor, best_cor)\n",
    "\n",
    "def get_best_image(rgb_img):\n",
    "    \n",
    "    shift_mse_1, _, shift_cor_1, _ = get_best_shift(rgb_img[:,:,0], rgb_img[:,:,1])\n",
    "    shift_mse_2, _, shift_cor_2, _ = get_best_shift(rgb_img[:,:,2], rgb_img[:,:,1])\n",
    "    \n",
    "    new_img = rgb_img.copy()\n",
    "    new_img[:,:,0] = shift_img(rgb_img[:,:,0], shift_mse_1)\n",
    "    new_img[:,:,2] = shift_img(rgb_img[:,:,2], shift_mse_2)\n",
    "    \n",
    "    return new_img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using MSE metrics\n",
    "final_imgs = []\n",
    "for img in rgb_imgs:\n",
    "    final_img = get_best_image(img)\n",
    "    final_imgs.append(final_img)\n",
    "\n",
    "visualize(final_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using COR Metrics\n",
    "final_imgs = []\n",
    "for img in rgb_imgs:\n",
    "    final_img = get_best_image(img)\n",
    "    final_imgs.append(final_img)\n",
    "\n",
    "visualize(final_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Alignment (2.5 points)\n",
    "\n",
    "In this task, you have to implement face normalization and alignment. Most of the face images deceptively seem to be aligned, but since many face recognition algorithms are very sensitive to shifts and rotations, we need not only to find a face on the image but also normalize it. Besides, the neural networks usually used for recognition have fixed input size, so, the normalized face images should be resized as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are six images of faces you have to normalize. In addition, you have the coordinates of the eyes in each of the pictures. You have to rotate the image so that the eyes are on the same height, crop the square box containing the face and transform it to the size $224\\times 224.$ The eyes should be located symmetrically and in the middle of the image (on the height)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how the transformation should look like.\n",
    "\n",
    "<img src = \"https://cdn1.savepice.ru/uploads/2017/12/13/286e475ef7a4f4e59005bcf7de78742f-full.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data\n",
    "You get the images and corresponding eyes coordinates for each person. You should implement the  function $\\tt{load}$\\_$\\tt{faces}$\\_$\\tt{and}$\\_$\\tt{eyes}$ that reads the data and returns two dictionaries: the dictionary of images and the dictionary of eyes coordinates. Eyes coordinates is a list of two tuples $[(x_1,y_1),(x_2,y_2)]$.\n",
    "Both dictionaries should have filenames as the keys.\n",
    "\n",
    "$\\tt{dir}$\\_$\\tt{name}$ is the path to the directory with face images, $\\tt{eye}$\\_$\\tt{path}$ is the path to .pickle file with eyes coordinates. If these directory and file are located in the same directory as this notebook, then default arguments can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import skimage\n",
    "\n",
    "def load_faces_and_eyes(dir_name = 'faces_imgs', eye_path = './eyes.pickle'):\n",
    "    \n",
    "    eyes = {}\n",
    "    imgs = {}\n",
    "    for filename in os.listdir(dir_name):\n",
    "        img = mpimg.imread(dir_name + '/' + filename)\n",
    "        imgs[filename] = img\n",
    "        \n",
    "    eye_coords = pickle.load(open(eye_path, 'rb'))\n",
    "    return imgs, eye_coords\n",
    "    \n",
    "    \n",
    "faces, eyes = load_faces_and_eyes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the input images look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(faces.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You may make the transformation using your own algorithm or by the following steps:\n",
    "1. Find the angle between the segment connecting two eyes and horizontal line;\n",
    "2. Rotate the image;\n",
    "3. Find the coordinates of the eyes on the rotated image\n",
    "4. Find the width and height of the box containing the face depending on the eyes coordinates\n",
    "5. Crop the box and resize it to $224\\times224$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# TODO: implement the function transform_face that rotates      #\n",
    "#       the image so that the eyes have equal ordinate,         #\n",
    "#       crops the square box containing face and resizes it.    #\n",
    "#       You can use methods from skimage library if you need.   #\n",
    "#       (2.5 points)                                              #\n",
    "#################################################################\n",
    "\n",
    "def transform_face(image, eyes):\n",
    "    (x1, y1), (x2, y2) = eyes\n",
    "\n",
    "    a = np.arctan((y2-y1)/(x2-x1)) # angle in radians\n",
    "    angle_deg = np.rad2deg(a) # angle in degrees\n",
    "    \n",
    "    rows, cols = image.shape[0], image.shape[1]\n",
    "    center_x = (cols / 2 - 0.5)\n",
    "    center_y =  rows / 2 - 0.5\n",
    "    \n",
    "    \n",
    "    #Rotation of the image\n",
    "    image = skimage.transform.rotate(image, angle_deg)\n",
    "    \n",
    "    # calculating the coordinates in the center of the image referential\n",
    "    #since the rotation is done centered on the center of the image\n",
    "    x1 = x1 - center_x\n",
    "    y1 = y1 - center_y\n",
    "    x2 = x2 - center_x\n",
    "    y2 = y2 - center_y\n",
    "\n",
    "    # calculate new coordinates\n",
    "    newX1 = x1 * cos(a) + y1 * sin(a)\n",
    "    newY1 = y1 * cos(a) - x1 * sin(a)\n",
    "    newX2 = x2 * cos(a) + y2 * sin(a)\n",
    "    newY2 = y2 * cos(a) - x2 * sin(a)\n",
    "    \n",
    "    # calculate coordonate in the original referential\n",
    "    newX1 = newX1 + center_x\n",
    "    newY1 = newY1 + center_y\n",
    "    \n",
    "    newX2 = newX2 + center_x\n",
    "    newY2 = newY2 + center_y\n",
    "    \n",
    "    \n",
    "    # cropping the image\n",
    "    new_size = np.int32(224)\n",
    "    new_size_center = new_size // 2\n",
    "\n",
    "    middle_eyes_x = np.int(newX1 + ((newX2 - newX1) // 2))\n",
    "    middle_eyes_y = np.int(newY1)\n",
    "    \n",
    "    start_x = max(0, middle_eyes_x - new_size_center)\n",
    "    end_x = middle_eyes_x + new_size_center\n",
    "    \n",
    "    start_y = max(0, middle_eyes_y-new_size_center)\n",
    "    end_y = middle_eyes_y + new_size_center\n",
    "    \n",
    "    \n",
    "    return image[start_y:end_y, start_x:end_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_imgs = []\n",
    "for i in faces:\n",
    "    img = faces[i]\n",
    "    eye = eyes[i]\n",
    "    transformed = transform_face(img, eye)\n",
    "    transformed_imgs.append(transformed)\n",
    "    \n",
    "visualize(transformed_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:27:43) \n[Clang 11.1.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "22c80f1cb90b85197dc3fdfdfdfc84c8d55cf8edc17193f44d31e8f8cae5800b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
